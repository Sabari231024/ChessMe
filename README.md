# â™Ÿï¸ ChessMe â€” Continual Learning Chess AI

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=flat&logo=pytorch&logoColor=white)](https://pytorch.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=flat&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![React](https://img.shields.io/badge/React-18+-61DAFB?style=flat&logo=react&logoColor=black)](https://reactjs.org)

> **A self-improving chess agent that learns from every game you play against it.**

ChessMe is an Actor-Critic reinforcement learning chess engine that continuously improves through gameplay. Unlike traditional chess engines that rely on pre-computed knowledge, ChessMe learns from your moves and its own mistakes â€” becoming stronger with each session.

---

## ğŸ¯ Project Vision

Build a chess AI that:
- **Learns like a human** â€” improves through post-game analysis
- **Remembers your style** â€” adapts to how you play
- **Never stops growing** â€” every game makes it stronger

---

## ğŸ§  How It Works

### Actor-Critic Architecture

The heart of ChessMe is an **Actor-Critic neural network** â€” a powerful reinforcement learning paradigm that combines two complementary learning signals:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ChessNet Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚     Input: 12Ã—8Ã—8 Board Tensor (piece positions)            â”‚
â”‚                        â”‚                                    â”‚
â”‚                        â–¼                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚     â”‚   Convolutional Feature Extractor â”‚                   â”‚
â”‚     â”‚   Conv2d(12â†’64) â†’ ReLU            â”‚                   â”‚
â”‚     â”‚   Conv2d(64â†’128) â†’ ReLU           â”‚                   â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                        â”‚                                    â”‚
â”‚                        â–¼                                    â”‚
â”‚              Flatten: 128Ã—8Ã—8 = 8192                        â”‚
â”‚                   /         \                               â”‚
â”‚                  /           \                              â”‚
â”‚                 â–¼             â–¼                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚   Policy Head   â”‚ â”‚   Value Head    â”‚                 â”‚
â”‚     â”‚   (Actor)       â”‚ â”‚   (Critic)      â”‚                 â”‚
â”‚     â”‚                 â”‚ â”‚                 â”‚                 â”‚
â”‚     â”‚ Linear(8192â†’    â”‚ â”‚ Linear(8192â†’1)  â”‚                 â”‚
â”‚     â”‚        4672)    â”‚ â”‚      â†“          â”‚                 â”‚
â”‚     â”‚      â†“          â”‚ â”‚    tanh()       â”‚                 â”‚
â”‚     â”‚  Softmax        â”‚ â”‚                 â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚            â”‚                    â”‚                           â”‚
â”‚            â–¼                    â–¼                           â”‚
â”‚     Move Probabilities    Position Value                    â”‚
â”‚     (4672 legal moves)    (-1 to +1)                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| Component | Purpose |
|-----------|---------|
| **Actor (Policy Head)** | Outputs probability distribution over all possible moves |
| **Critic (Value Head)** | Estimates how good the current position is (win probability) |

### Why Actor-Critic?

| Approach | Limitation |
|----------|------------|
| Policy-only (REINFORCE) | High variance, unstable learning |
| Value-only (DQN) | Can't handle continuous/large action spaces well |
| **Actor-Critic** | Best of both â€” stable, efficient, scalable âœ“ |

---

## ğŸ”„ Continual Learning Loop

```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                                 â”‚
    â–¼                                                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ Play a  â”‚â”€â”€â”€â–¶â”‚ Store Game  â”‚â”€â”€â”€â–¶â”‚ Train Model   â”‚â”€â”€â”€â”˜
â”‚ Game    â”‚    â”‚ (memory.py) â”‚    â”‚ (train.py)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                    â”‚
                     â”‚                    â–¼
                     â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚          â”‚ Next Game:      â”‚
                     â”‚          â”‚ Stronger AI!    â”‚
                     â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
            Game data includes:
            â€¢ All board states
            â€¢ All moves made
            â€¢ Final result (+1/-1)
```

### Learning Process

1. **You play a game** against the AI (as white or black)
2. **Game is recorded** â€” every position, every move, final outcome
3. **Post-game training** â€” the model replays the game multiple times:
   - **Policy head** learns: "In position X, move Y won/lost the game"
   - **Value head** learns: "Position X led to a win/loss"
4. **Next session** â€” the AI plays with updated knowledge

> ğŸ’¡ *This is how humans improve: analyze your games, learn from mistakes, do better next time.*

---

## âœ¨ Key Features

### âœ… Currently Implemented

| Feature | Description |
|---------|-------------|
| **Legal Move Masking** | AI only considers valid chess moves â€” no probability wasted on illegal moves |
| **Temperature-Based Selection** | Controls exploration vs exploitation (higher = more varied play) |
| **Game Memory System** | Persists game data between sessions for continual learning |
| **Full Action Space** | Supports all 4,672 possible moves including promotions |
| **GPU Acceleration** | CUDA support for fast training and inference |
| **Web Interface** | Beautiful React chessboard with drag-and-drop |
| **FastAPI Backend** | Production-ready API for AI moves |

### ğŸš§ Phase 3 â€” Coming Next

| Feature | Benefit |
|---------|---------|
| **Entropy Regularization** | Prevents overfitting to specific games |
| **Enhanced Temperature Scheduling** | Smarter exploration during training |
| **Self-Play Mode** | AI plays against itself for accelerated learning |

---

## ğŸ“ Project Structure

```
ChessMe/
â”œâ”€â”€ app/                          # Backend (Python)
â”‚   â”œâ”€â”€ model.py                  # ChessNet: Actor-Critic neural network
â”‚   â”œâ”€â”€ ai_player.py              # Move selection with legal masking
â”‚   â”œâ”€â”€ train.py                  # Training loop (learns from games)
â”‚   â”œâ”€â”€ memory.py                 # Game persistence (pickle storage)
â”‚   â”œâ”€â”€ board_utils.py            # Board â†’ Tensor conversion
â”‚   â”œâ”€â”€ action_utils.py           # Move encoding/decoding (4672 actions)
â”‚   â”œâ”€â”€ main.py                   # FastAPI server
â”‚   â””â”€â”€ play.py                   # CLI play mode
â”‚
â”œâ”€â”€ frontend/                     # Frontend (React + Vite)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ App.jsx               # Chess UI with react-chessboard
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ models/                       # Saved model weights
â”‚   â””â”€â”€ latest_model.pt           # Current best model
â”‚
â”œâ”€â”€ data/                         # Game storage
â”‚   â””â”€â”€ last_game.pkl             # Most recent game for training
â”‚
â””â”€â”€ requirements.txt              # Python dependencies
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Node.js 18+ (for frontend)
- CUDA-capable GPU (optional, but recommended)

### Backend Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/ChessMe.git
cd ChessMe

# Create virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# Install dependencies
pip install torch python-chess fastapi uvicorn pydantic numpy

# Start the API server
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

### Frontend Setup

```bash
# In a new terminal
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

Visit **http://localhost:5173** to play!

---

## ğŸ® How to Play

1. **Start both servers** (backend on :8000, frontend on :5173)
2. **Open the web UI** in your browser
3. **Play as White** â€” drag and drop pieces
4. **AI responds** â€” the model calculates and plays its move
5. **After the game** â€” run training to improve the AI:

```bash
python -m app.train
```

---

## ğŸ”§ Configuration

### Temperature (in `ai_player.py`)

```python
ai_select_move(board, temperature=1.0)
```

| Value | Behavior |
|-------|----------|
| `0.1` | Deterministic â€” always plays "best" move |
| `1.0` | Balanced â€” probabilistic selection |
| `2.0` | Exploratory â€” more random, creative moves |

### Training Parameters (in `train.py`)

```python
train_on_last_game(epochs=20, lr=1e-4)
```

| Parameter | Default | Description |
|-----------|---------|-------------|
| `epochs` | 20 | Times to replay the game |
| `lr` | 1e-4 | Learning rate (lower = more stable) |

---

## ğŸ“Š Technical Details

### Board Representation

The board is encoded as a **12Ã—8Ã—8 tensor**:
- 6 channels for White pieces (Pawn, Knight, Bishop, Rook, Queen, King)
- 6 channels for Black pieces
- Binary encoding: 1 = piece present, 0 = empty

### Action Space

| Moves | Description |
|-------|-------------|
| 4,672 | All possible (from_square, to_square, promotion) combinations |

Legal move masking ensures invalid moves get probability â‰ˆ 0.

### Loss Function

```
Total Loss = Policy Loss + 0.5 Ã— Value Loss

Policy Loss = CrossEntropy(predicted_move, actual_move)
Value Loss  = (predicted_value - game_result)Â²
```

---

## ğŸ§ª Training Tips

1. **Play lots of games** â€” more data = better learning
2. **Complete games** â€” resigned games still provide valuable signal
3. **Mix play styles** â€” occasionally explore unusual moves
4. **Train incrementally** â€” run training after every few games

---

## ğŸ›£ï¸ Roadmap

### Phase 1 âœ… â€” Foundation
- [x] Actor-Critic network architecture
- [x] Board state encoding
- [x] Legal move masking
- [x] Game memory system
- [x] FastAPI + React integration

### Phase 2 âœ… â€” Core Learning
- [x] Training from recorded games
- [x] Temperature-based move selection
- [x] Model persistence
- [x] Gradient clipping for stability

### Phase 3 ğŸ”œ â€” Enhancement
- [ ] Entropy regularization
- [ ] Advanced temperature scheduling
- [ ] Opening book integration
- [ ] Evaluation mode (deterministic play)

### Phase 4 ğŸ”® â€” Advanced
- [ ] Self-play training
- [ ] Experience replay buffer
- [ ] Multi-game batch training
- [ ] ELO tracking system

---

## ğŸ§¬ Why This Approach Works

Traditional chess engines (like Stockfish) use:
- Hand-crafted evaluation functions
- Alpha-beta pruning
- Massive opening books

**ChessMe is different:**
- Learns from scratch through gameplay
- Discovers patterns naturally
- Adapts to opponent styles
- Gets better over time â€” not static

> This is the same core idea behind **AlphaGo/AlphaZero**, simplified for personal use.

---

## ğŸ“š References

- [Actor-Critic Methods](https://arxiv.org/abs/1602.01783) â€” Mnih et al.
- [AlphaZero Paper](https://arxiv.org/abs/1712.01815) â€” Silver et al.
- [python-chess Library](https://python-chess.readthedocs.io/)
- [PyTorch Documentation](https://pytorch.org/docs/)

---

## ğŸ¤ Contributing

Contributions are welcome! Areas that need work:
- Improved board evaluation
- Self-play infrastructure
- Mobile-friendly UI
- Game analysis tools

---

## ğŸ“„ License

MIT License â€” feel free to use, modify, and share.

---

<div align="center">

**Made with â™Ÿï¸ and ğŸ§  by the ChessMe team**

*Teach your AI to play chess â€” one game at a time.*

</div>

ReadMe File created by Antigravity AI
